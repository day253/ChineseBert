{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets.bert_mask_dataset import BertMaskDataset\n",
    "from models.modeling_glycebert import GlyceBertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_path = \"/root/.cache/huggingface/hub/models--ShannonAI--ChineseBERT-base/snapshots/aa8b6fa9c3427f77b0911b07ab35f2b1b8bf248b\"\n",
    "\n",
    "# step 1: mask sentence\n",
    "vocab_file = os.path.join(pretrain_path, 'vocab.txt')\n",
    "config_path = os.path.join(pretrain_path, 'config')\n",
    "tokenizer = BertMaskDataset(vocab_file, config_path)\n",
    "\n",
    "# step 2: load model\n",
    "chinese_bert = GlyceBertForMaskedLM.from_pretrained(pretrain_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: [CLS] [MASK] 喜 欢 小 猫 [SEP]\n",
      "output sentence: [CLS] 我 喜 欢 小 猫 [SEP]\n",
      "\n",
      "input sentence: [CLS] 我 [MASK] 欢 小 猫 [SEP]\n",
      "output sentence: [CLS] 我 喜 欢 小 猫 [SEP]\n",
      "\n",
      "input sentence: [CLS] 我 喜 [MASK] 小 猫 [SEP]\n",
      "output sentence: [CLS] 我 喜 欢 小 猫 [SEP]\n",
      "\n",
      "input sentence: [CLS] 我 喜 欢 [MASK] 猫 [SEP]\n",
      "output sentence: [CLS] 我 喜 欢 熊 猫 [SEP]\n",
      "\n",
      "input sentence: [CLS] 我 喜 欢 小 [MASK] [SEP]\n",
      "output sentence: [CLS] 我 喜 欢 小 。 [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"我喜欢小猫\"\n",
    "\n",
    "# step 3: mask each position and fill\n",
    "for i in range(len(sentence)):\n",
    "    input_ids, pinyin_ids = tokenizer.mask_sentence(sentence, i)\n",
    "    length = input_ids.shape[0]\n",
    "    input_ids = input_ids.view(1, length)\n",
    "    pinyin_ids = pinyin_ids.view(1, length, 8)\n",
    "    output = chinese_bert.forward(input_ids, pinyin_ids)[0]\n",
    "    predict_labels = torch.argmax(output, dim=-1)[0]\n",
    "    predict_label = predict_labels[i + 1].item()\n",
    "    output_ids = input_ids.numpy()[0].tolist()\n",
    "    output_ids[i + 1] = predict_label\n",
    "\n",
    "    input_sentence = tokenizer.tokenizer.decode(input_ids.numpy().tolist()[0], skip_special_tokens=False)\n",
    "    output_sentence = tokenizer.tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "    print(\"input sentence:\", input_sentence)\n",
    "    print(\"output sentence:\", output_sentence)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
